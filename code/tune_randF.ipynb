{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Training Data Set\n",
    "df_train_1 = pd.read_csv(r'Data\\Train\\Data_illustrated_CSV.csv')\n",
    "\n",
    "df_train_c2 = pd.read_csv(r'Data\\Train\\corridor_CSV\\July22_14.csv')                  \n",
    "df_train_c3 = pd.read_csv(r'Data\\Train\\corridor_CSV\\July22_16.csv')  \n",
    "df_train_c4 = pd.read_csv(r'Data\\Train\\corridor_CSV\\July22_16.csv')                  \n",
    "df_train_c5 = pd.read_csv(r'Data\\Train\\corridor_CSV\\July22_18.csv')                  \n",
    "df_train_c6 = pd.read_csv(r'Data\\Train\\corridor_CSV\\July22_20.csv')                  \n",
    "\n",
    "df_train_b1 = pd.read_csv(r'Data\\Train\\Open_Box_CSV_files\\Aug14_Box_2.csv')    \n",
    "df_train_b2 = pd.read_csv(r'Data\\Train\\Open_Box_CSV_files\\Aug14_Box_4.csv')    \n",
    "df_train_b3 = pd.read_csv(r'Data\\Train\\Open_Box_CSV_files\\Aug14_Box_7.csv')    \n",
    "df_train_b4 = pd.read_csv(r'Data\\Train\\Open_Box_CSV_files\\Aug14_Box_8.csv')    \n",
    "df_train_b5 = pd.read_csv(r'Data\\Train\\Open_Box_CSV_files\\Aug14_Box_9.csv')    \n",
    "df_train_b6 = pd.read_csv(r'Data\\Train\\Open_Box_CSV_files\\Aug14_Box_17.csv')    \n",
    "df_train_b7 = pd.read_csv(r'Data\\Train\\Open_Box_CSV_files\\Aug14_Box_18.csv')    \n",
    "\n",
    "df_train_11 = pd.read_csv(r'Data\\Train\\special_CSV\\Aug16_Box_special_1.csv')    \n",
    "\n",
    "# create datasets\n",
    "df_train_1.columns = df_train_c2.columns = df_train_c3.columns = df_train_c4.columns  = df_train_c5.columns\n",
    "df_train_b1.columns = df_train_b2.columns = df_train_b3.columns = df_train_b4.columns = df_train_b5.columns = df_train_b6.columns\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "# Import Testing Data Set\n",
    "df_test_1 = pd.read_csv(r'Data\\Test\\Aug14_Box_g1.csv')\n",
    "df_test_2 = pd.read_csv(r'Data\\Test\\Aug14_Box_g2.csv')\n",
    "df_test_3 = pd.read_csv(r'Data\\Test\\Aug14_Box_3.csv')\n",
    "df_test_4 = pd.read_csv(r'Data\\Test\\Aug14_Box_5.csv')\n",
    "df_test_5 = pd.read_csv(r'Data\\Test\\Aug14_Box_11.csv')\n",
    "\n",
    "df_test_6 = pd.read_csv(r'Data\\Test\\July22_22.csv')\n",
    "df_test_7 = pd.read_csv(r'Data\\Test\\July22_33.csv')\n",
    "df_test_8 = pd.read_csv(r'Data\\Test\\July22_66.csv')\n",
    "\n",
    "# df_test_1.columns = df_test_2.columns =\n",
    "df_test_3.columns = df_test_4.columns = df_test_5.columns\n",
    "df_test_6.columns = df_test_7.columns = df_test_8.columns\n",
    "\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible training set\n",
    "box_train = [df_train_b1, df_train_b2, df_train_b3, df_train_b4, df_train_b5, df_train_b6]\n",
    "corridor_train = [df_train_1, df_train_c2, df_train_c3,df_train_c4,df_train_c5]\n",
    "\n",
    "# possible test set\n",
    "box_test = [df_test_3, df_test_4, df_test_5]\n",
    "corridor_test =  [df_test_6, df_test_7, df_test_8]\n",
    "\n",
    "# ==================================================\n",
    "# ==================================================\n",
    "df_train = box_train\n",
    "df_test = box_test\n",
    "# ==================================================\n",
    "# =================================================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      " laser features shape -->  (118623, 18)\n",
      " total training features shape -->  (118623, 26)\n",
      " total training translation velocity predictions shape -->  (118623, 1)\n",
      " total training angular velocity predictions shape -->  (118623, 1)\n",
      "==================================================================\n"
     ]
    }
   ],
   "source": [
    "# Combine Training Set                 \n",
    "df_training = pd.concat(df_train, axis=0, ignore_index=True)\n",
    "\n",
    "# Shuffle Dataset\n",
    "df_training = shuffle(df_training)\n",
    "\n",
    "# Reduce Laser Data\n",
    "laser_df = pd.DataFrame()\n",
    "bins_sz = 15*4\n",
    "bin_list = np.arange(0,1080,bins_sz)\n",
    "for c in bin_list:    # loop every 40 columns\n",
    "    laser_df[c/bins_sz] = df_training.iloc[:,c:c+bins_sz].astype(float).mean(axis=1)\n",
    "laser_df.shape\n",
    "\n",
    "# Extract Useful Features [laser, local goal, pose]\n",
    "laser = np.array(laser_df)  \n",
    "local_goal = np.array(df_training)[:,1084:1088]                  \n",
    "pose = np.array(df_training)[:,1088:1092]                  \n",
    "features = np.concatenate((laser,local_goal,pose), axis=1)\n",
    "\n",
    "# Extract Predictions\n",
    "cmd_vel = np.array(df_training)[:,1092:]    \n",
    "\n",
    "# Initialize Training Set Inputs\n",
    "X_training = features\n",
    "y_training = cmd_vel\n",
    "y_training_vel = cmd_vel[:,0:1]\n",
    "y_training_ang = cmd_vel[:,1:]\n",
    "\n",
    "# Print Training Input Shapes\n",
    "print(\"==================================================================\")\n",
    "print(\" laser features shape --> \", laser.shape)  \n",
    "print(\" total training features shape --> \", X_training.shape)  \n",
    "print(\" total training translation velocity predictions shape --> \", y_training_vel.shape)  \n",
    "print(\" total training angular velocity predictions shape --> \", y_training_ang.shape) \n",
    "print(\"==================================================================\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training / Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      " training features shape -->  (88967, 26)\n",
      " training translation velocity predictions shape -->  (88967, 1)\n",
      " training angular velocity predictions shape -->  (88967, 1)\n",
      "==================================================================\n",
      " validation features shape -->  (29656, 26)\n",
      " validation translation velocity predictions shape -->  (29656, 1)\n",
      " validation angular velocity predictions shape -->  (29656, 1)\n",
      "==================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_training, y_training, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "y_train_vel = y_train[:,0:1]\n",
    "y_train_ang = y_train[:,1:]\n",
    "\n",
    "y_val_vel = y_val[:,0:1]\n",
    "y_val_ang = y_val[:,1:]\n",
    "\n",
    "# Print Training Input Shapes\n",
    "print(\"==================================================================\")\n",
    "print(\" training features shape --> \", X_train.shape)  \n",
    "print(\" training translation velocity predictions shape --> \", y_train_vel.shape)  \n",
    "print(\" training angular velocity predictions shape --> \", y_train_ang.shape) \n",
    "print(\"==================================================================\")\n",
    "# Print Training Input Shapes\n",
    "print(\" validation features shape --> \", X_val.shape)  \n",
    "print(\" validation translation velocity predictions shape --> \", y_val_vel.shape)  \n",
    "print(\" validation angular velocity predictions shape --> \", y_val_ang.shape) \n",
    "print(\"==================================================================\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      " test features shape -->  (85837, 26)\n",
      " test translation velocity predictions shape -->  (85837, 1)\n",
      " test angular velocity predictions shape -->  (85837, 1)\n",
      "==================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_test = pd.concat(df_test, axis=0, ignore_index=True)\n",
    "\n",
    "# Reduce Laser Data\n",
    "laser_df_test = pd.DataFrame()\n",
    "for c in bin_list:    # loop every 40 columns\n",
    "    laser_df_test[c/bins_sz] = df_test.iloc[:,c:c+bins_sz].astype(float).mean(axis=1)\n",
    "laser_df_test.shape\n",
    "                  \n",
    "# Extract Useful Features [laser, local goal, pose]\n",
    "laser = np.array(laser_df_test)  \n",
    "local_goal = np.array(df_test)[:,1084:1088]                  \n",
    "pose = np.array(df_test)[:,1088:1092]                  \n",
    "features = np.concatenate((laser,local_goal,pose), axis=1)\n",
    "\n",
    "# Extract Predictions\n",
    "cmd_vel = np.array(df_test)[:,1092:]  \n",
    "\n",
    "# Initialize Testing Set Inputs\n",
    "X_test = features\n",
    "y_test = cmd_vel\n",
    "\n",
    "y_test_vel = cmd_vel[:,0:1]\n",
    "y_test_ang = cmd_vel[:,1:]\n",
    "\n",
    "# Print Testing Input Shapes\n",
    "print(\"==================================================================\")\n",
    "print(\" test features shape --> \", X_test.shape)  \n",
    "print(\" test translation velocity predictions shape --> \", y_test_vel.shape)  \n",
    "print(\" test angular velocity predictions shape --> \", y_test_ang.shape) \n",
    "print(\"==================================================================\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scale = StandardScaler()\n",
    "scale.fit(X_train)\n",
    "\n",
    "x_train_scaled = scale.transform(X_train)\n",
    "x_test_scaled =  scale.transform(X_test)\n",
    "x_val_scaled =  scale.transform(X_val)\n",
    "\n",
    "\n",
    "# Comment out to predict v and w together\n",
    "y_train = y_train_vel\n",
    "y_test = y_test_vel\n",
    "y_val = y_val_vel \n",
    "\n",
    "# Comment out to predict v and w together\n",
    "# y_train = y_train_ang\n",
    "# y_test = y_test_ang\n",
    "# y_val = y_val_ang "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jai Sharma\\AppData\\Local\\Temp\\ipykernel_34496\\1203128136.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  forest.fit(x_train_scaled, y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train: 0.0002, test: 0.1325\n",
      "R^2 train: 0.9987, test: 0.0282\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training\n",
    "forest = RandomForestRegressor(n_estimators = 30, random_state=30)\n",
    "forest.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred_lr_train = forest.predict(x_train_scaled)\n",
    "y_pred_lr_test = forest.predict(x_test_scaled)\n",
    "\n",
    "# In Sample and Out of Sample Error [MSE and R^2]\n",
    "print('MSE train: %.4f, test: %.4f' % (mean_squared_error(y_train, y_pred_lr_train), mean_squared_error(y_test, y_pred_lr_test)))\n",
    "print('R^2 train: %.4f, test: %.4f' % (r2_score(y_train, y_pred_lr_train), r2_score(y_test, y_pred_lr_test)))\n",
    "\n",
    "# forest.get_params()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.model_selection import learning_curve\n",
    "\n",
    "# train_sizes, train_scores, test_scores = learning_curve(estimator=forest, X=x_train_scaled, y=y_train_vel.ravel(),\n",
    "#                                                        cv=10, train_sizes=np.linspace(0.1, 1.0, 10, 100),\n",
    "#                                                        n_jobs=1)\n",
    "# # Calculate training and test mean and std\n",
    "# train_mean = np.mean(train_scores, axis=1)\n",
    "# train_std = np.std(train_scores, axis=1)\n",
    "# test_mean = np.mean(test_scores, axis=1)\n",
    "# test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# # Plot the learning curve\n",
    "# plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='Training Accuracy')\n",
    "# plt.plot(train_sizes, test_mean, color='green', marker='+', markersize=5, linestyle='--', label='Validation Accuracy')\n",
    "# plt.title('Learning Curve')\n",
    "# plt.xlabel('Training Data Size')\n",
    "# plt.ylabel('Model accuracy')\n",
    "# plt.grid()\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.datasets import load_energy\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "# Instantiate the regression model and visualizer\n",
    "visualizer = LearningCurve(forest, scoring='r2')\n",
    "\n",
    "# visualizer.fit(x_train_scaled, y_train.ravel())        # Fit the data to the visualizer\n",
    "# visualizer.show()           # Finalize and render the figure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Random Forest Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 20, stop = 300, num = 10)] # Number of trees in random forest\n",
    "max_features = ['auto', 'sqrt']                                       # Number of features to consider at every split\n",
    "max_depth = [int(x) for x in np.linspace(0, 700, num = 11)]          # Maximum number of levels in tree\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]        # Minimum number of samples required to split a node\n",
    "min_samples_leaf = [1, 2, 4]          # Minimum number of samples required at each leaf node\n",
    "bootstrap = [True, False]             # Method of selecting samples for training each tree\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "                # 'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "            #    'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.9601733235550816\n",
      "best_param:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'min_samples_split': 2,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 490,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparam Tuned on Validation set\n",
    "\n",
    "bestForest = RandomizedSearchCV(estimator = forest, param_distributions = random_grid, n_iter = 10, cv = 3, random_state=42, n_jobs = -1)\n",
    "bestForest.fit(x_val_scaled, y_val.ravel())\n",
    "print(\"best score:\", bestForest.best_score_)\n",
    "print(\"best_param:\")\n",
    "bestForest.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train: 0.0031, test: 0.1226\n",
      "R^2 train: 0.9780, test: 0.1012\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "y_pred_lr_train = bestForest.predict(x_train_scaled)\n",
    "y_pred_lr_test = bestForest.predict(x_test_scaled)\n",
    "\n",
    "# In Sample and Out of Sample Error [MSE and R^2]\n",
    "print('MSE train: %.4f, test: %.4f' % (mean_squared_error(y_train, y_pred_lr_train), mean_squared_error(y_test, y_pred_lr_test)))\n",
    "print('R^2 train: %.4f, test: %.4f' % (r2_score(y_train, y_pred_lr_train), r2_score(y_test, y_pred_lr_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e9be2e68cb295c361abd1a53a85653cf92a0e7d31e816aee4dac2509103d9a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
